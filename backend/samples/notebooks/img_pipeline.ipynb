{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Preprocessing Pipeline Demo\n",
    "\n",
    "This notebook demonstrates step-by-step how to execute the image preprocessing pipeline.  \n",
    "It loads an example image, applies the defined pipeline, and displays the before‑and‑after results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tested in python >= 3.12, < 3.13, use main pyproject.toml\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# En notebooks usamos os.getcwd() en lugar de __file__\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../../'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import matplotlib.pyplot as plt\n",
    "from lib.image.pipeline import process_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This preprocessing pipeline comprises a sequence of modular functions that `(1) deskew scanned images`, `(2) remove noise`, `(3) normalize brightness and contrast`, `(4) resize to a standard width`, `(5) enhance local contrast`, `(6) crop away empty borders`, `(7) generate an edge‑difference map`, `(8) extract embedded text`, `(9) record provenance metadata`, and `(10) compute a Keccak‑256 hash` to cryptographically “stamp” each processed image, ensuring both visual consistency and verifiable integrity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Modular Configuration \n",
    "\n",
    "```python\n",
    "# Pipeline configuration\n",
    "PIPELINE_STEPS = [\n",
    "    correct_image_rotation,\n",
    "    make_denoise(10, 7),\n",
    "    lambda img: adjust_brightness_contrast(img, alpha=1.2, beta=10),\n",
    "    resize_image,\n",
    "    normalize_image,\n",
    "    auto_crop\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Function Explanations\n",
    "\n",
    "1. **correct_image_rotation**  \n",
    "   Automatically straightens tilted document scans by binarizing via Otsu’s method, finding the minimum-area bounding rectangle of the foreground, and applying an affine rotation to align text horizontally.  \n",
    "   **References:** [Medium](https://medium.com), [Stack Overflow](https://stackoverflow.com).\n",
    "\n",
    "2. **make_denoise / Denoising**  \n",
    "   Uses Non-Local Means filtering to suppress random noise while preserving crucial edge details, by averaging pixels with similar neighborhood patterns across the image.  \n",
    "   **References:** [OpenCV Documentation](https://docs.opencv.org), [amroamroamro.github.io](https://amroamroamro.github.io).\n",
    "\n",
    "3. **adjust_brightness_contrast**  \n",
    "   Applies a linear intensity transform (output = α·input + β) via `cv2.convertScaleAbs()`, standardizing exposure and contrast across all images for greater visual uniformity.  \n",
    "   **References:** [TutorialsPoint](https://www.tutorialspoint.com), [OpenCV Documentation](https://docs.opencv.org).\n",
    "\n",
    "4. **resize_image**  \n",
    "   Rescales the image to a fixed width (e.g., 1024 px) while maintaining its aspect ratio, using area-based interpolation to avoid distortion and ensure consistent dimensions.  \n",
    "   **References:** [PyImageSearch](https://pyimagesearch.com), [Stack Overflow](https://stackoverflow.com).\n",
    "\n",
    "5. **normalize_image (CLAHE)**  \n",
    "   Enhances local contrast by dividing the image into contextual tiles and applying Contrast Limited Adaptive Histogram Equalization (CLAHE), thereby boosting detail in both dark and bright regions without over-amplifying noise.  \n",
    "   **References:** [GeeksforGeeks](https://www.geeksforgeeks.org), [PyImageSearch](https://pyimagesearch.com).\n",
    "\n",
    "6. **auto_crop**  \n",
    "   Removes blank margins by applying Otsu’s thresholding to separate foreground from background and cropping to the bounding box of foreground pixels, producing a tightly framed content area.  \n",
    "   **References:** [OpenCV Documentation](https://docs.opencv.org), [LearnOpenCV](https://learnopencv.com).\n",
    "\n",
    "7. **detect_edges**  \n",
    "   Generates a simple edge map using Canny edge detection, highlighting regions of high gradient that can later serve as a “difference mask” to illustrate pixel-level changes introduced during processing.  \n",
    "   **References:** [OpenCV Documentation](https://docs.opencv.org), [GeeksforGeeks](https://www.geeksforgeeks.org).\n",
    "\n",
    "8. **extract_text_from_image**  \n",
    "   Leverages Pytesseract, a Python wrapper for the Tesseract-OCR engine, to convert visible text in the image into editable string data, enabling semantic analysis of embedded labels or annotations.  \n",
    "   **References:** [AI Document Processing](https://ai.googleblog.com), [Stack Overflow](https://stackoverflow.com).\n",
    "\n",
    "9. **generate_metadata**  \n",
    "   Assembles a structured record of each processing step—such as input/output dimensions, blur detection flags, and applied parameters—providing provenance metadata that supports reproducibility and auditing.  \n",
    "   **References:** [LDFI](https://ldfi.io), [exterro.com](https://www.exterro.com).\n",
    "\n",
    "10. **keccak_image_hash**  \n",
    "    Encodes the final processed image (e.g., as PNG bytes) and computes a Keccak-256 digest—a cryptographic hash standard used in blockchain systems—to produce a unique, tamper-evident fingerprint for each image.  \n",
    "    **References:** [GeeksforGeeks](https://www.geeksforgeeks.org), [ethereum.stackexchange.com](https://ethereum.stackexchange.com).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@905.064] global loadsave.cpp:268 findDecoder imread_('/home/xgen0/CertyChain/CertyLex/backend/data/images/img_pipeline/rotated.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No se pudo cargar /home/xgen0/CertyChain/CertyLex/backend/data/images/img_pipeline/rotated.jpg",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m image_path = \u001b[33m'\u001b[39m\u001b[33mrotated.jpg\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      4\u001b[39m image_path = os.path.join(image_dir, image_path)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m result = \u001b[43mprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m orig = result[\u001b[33m'\u001b[39m\u001b[33moriginal\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      7\u001b[39m proc = result[\u001b[33m'\u001b[39m\u001b[33mprocessed\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CertyChain/CertyLex/backend/lib/image/pipeline.py:196\u001b[39m, in \u001b[36mprocess_image\u001b[39m\u001b[34m(image_path, steps)\u001b[39m\n\u001b[32m    194\u001b[39m orig = cv2.imread(image_path)\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo se pudo cargar \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    198\u001b[39m image = orig.copy()\n\u001b[32m    199\u001b[39m history = []\n",
      "\u001b[31mValueError\u001b[39m: No se pudo cargar /home/xgen0/CertyChain/CertyLex/backend/data/images/img_pipeline/rotated.jpg"
     ]
    }
   ],
   "source": [
    "# Load and process an example image\n",
    "image_dir = os.path.join(project_root, 'samples', 'data', 'images', 'img_pipeline')\n",
    "image_path = 'rotated.jpg'\n",
    "image_path = os.path.join(image_dir, image_path)\n",
    "result = process_image(image_path)\n",
    "orig = result['original']\n",
    "proc = result['processed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image preprocessing + metadata parsing\n",
    "\n",
    "```python\n",
    "def process_image(image_path, steps=None):\n",
    "    \"\"\"\n",
    "    Process an image using a series of image processing steps and return results.\n",
    "    Args:\n",
    "        image_path (str): The path to the input image.\n",
    "        steps (list): A list of functions to apply to the image.\n",
    "    Returns:\n",
    "        dict: Contains original image, processed image, metadata, history, and hash.\n",
    "    \"\"\"\n",
    "    orig = cv2.imread(image_path)\n",
    "    if orig is None:\n",
    "        raise ValueError(f\"No se pudo cargar {image_path}\")\n",
    "\n",
    "    image = orig.copy()\n",
    "    history = []\n",
    "    metadata = {}\n",
    "    if steps is None:\n",
    "        steps = PIPELINE_STEPS\n",
    "\n",
    "    for step in steps:\n",
    "        try:\n",
    "            prev = image.copy()\n",
    "            image = step(image)\n",
    "            metadata[step.__name__] = {\n",
    "                \"input_shape\": prev.shape,\n",
    "                \"output_shape\": image.shape\n",
    "            }\n",
    "            history.append(f\"{step.__name__} applied\")\n",
    "        except Exception as e:\n",
    "            history.append(f\"{step.__name__} failed: {e}\")\n",
    "\n",
    "    # Compute final metadata and hash\n",
    "    metadata.update(generate_metadata(orig, image))\n",
    "    img_hash = keccak_image_hash(image)\n",
    "    metadata[\"hash\"] = img_hash\n",
    "    history.append(f\"hash computed: {img_hash[:8]}…\")\n",
    "\n",
    "    return {\n",
    "        \"original\": orig,\n",
    "        \"processed\": image,\n",
    "        \"metadata\": metadata,\n",
    "        \"history\": history\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display original vs processed images side by side\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Original Image')\n",
    "plt.imshow(cv2.cvtColor(orig, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Processed Image')\n",
    "plt.imshow(cv2.cvtColor(proc, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View metadata and processing history\n",
    "import pprint\n",
    "pprint.pprint(result['metadata'])\n",
    "\n",
    "print('\\nProcessing history:')\n",
    "for step in result['history']:\n",
    "    print('-', step)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "certylex-api-o2NBLT6Z-py3.12 (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
