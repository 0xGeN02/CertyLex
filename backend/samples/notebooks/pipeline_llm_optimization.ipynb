{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a14161f8",
   "metadata": {},
   "source": [
    "# Image Processing Pipeline Optimization Notebook\n",
    "This notebook demonstrates how to set up and optimize an image processing pipeline using a large language model (LLM) feedback loop. Each section includes explanations for a university-level understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fd71fa",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Imports\n",
    "We import necessary libraries and configure the environment to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d911c888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for environment management and scientific computing\n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import random\n",
    "import pytesseract  # OCR tool\n",
    "\n",
    "# Print versions for reproducibility\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d09d4a3",
   "metadata": {},
   "source": [
    "## 2. Project Root Path Configuration\n",
    "We add the project root to `sys.path` for module imports from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf76b452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust project root to import custom modules\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../../'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "print(f\"Project root added: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768f7704",
   "metadata": {},
   "source": [
    "## 3. LLM Pipeline Imports\n",
    "We import the optimized pipeline builders and processing functions from our `lib` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cd8588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.image.llm_pipeline import optimize_pipeline, build_pipeline, process_image\n",
    "from ollama import chat  # Ollama client for LLM interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0ce9b8",
   "metadata": {},
   "source": [
    "## 4. Defining LLM Models and Parameter Space\n",
    "We specify which LLMs to test and define the hyperparameter search space for each image processing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7fedc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM models available for feedback\n",
    "models = [\n",
    "    \"llama3.2-vision:11b\",\n",
    "    \"deepseek-r1:8b\",\n",
    "    \"deepseek-r1:32b\"\n",
    "]\n",
    "\n",
    "# Hyperparameter search ranges for each step\n",
    "param_space = {\n",
    "    \"denoise\": {\"h\": [5, 10, 15], \"template_window\": [3, 7]},\n",
    "    \"contrast\": {\"alpha\": [1.0, 1.2, 1.5], \"beta\": [0, 10, 20]},\n",
    "    \"resize\": {\"width\": [800, 1024, 1280]},\n",
    "    \"normalize\": {\"clipLimit\": [1.0, 2.0], \"tileGrid\": [(8, 8)]},\n",
    "    \"crop\": {\"margin\": [0, 5, 10]},\n",
    "    \"hash\": {}  # Placeholder for future steps\n",
    "}\n",
    "print(param_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9438370b",
   "metadata": {},
   "source": [
    "## 5. Generating Random Configuration\n",
    "Function to sample a random valid configuration from the parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08dc657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_config():\n",
    "    return {\n",
    "        \"use\": {\n",
    "            \"rotate\": random.choice([True, False]),\n",
    "            \"denoise\": True,\n",
    "            \"contrast\": random.choice([True, False]),\n",
    "            \"resize\": True,\n",
    "            \"normalize\": True,\n",
    "            \"crop\": random.choice([True, False]),\n",
    "            \"hash\": True\n",
    "        },\n",
    "        \"params\": {\n",
    "            \"denoise\": {\"h\": 10, \"template_window\": 7},\n",
    "            \"contrast\": {\"alpha\": 1.2, \"beta\": 10},\n",
    "            \"resize\": {\"width\": 1024},\n",
    "            \"normalize\": {\"clipLimit\": 2.0, \"tileGrid\": (8, 8)},\n",
    "            \"crop\": {\"margin\": 5}\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Example random config\n",
    "print(generate_random_config())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4347a1e4",
   "metadata": {},
   "source": [
    "## 6. Defining the LLM Prompt Template\n",
    "Template for requesting adapted pipeline parameters based on image metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec48f5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how the prompt is structured for the LLM to decide next steps\n",
    "prompt_template = '''\n",
    "An image was processed and the following metrics were evaluated:\n",
    "- Blur reduction: {blur_reduction:.2f}\n",
    "- OCR gain: {ocr_gain}\n",
    "- Overall score: {score:.2f}\n",
    "\n",
    "Initial parameters:\n",
    "{params_json}\n",
    "\n",
    "Decide which steps to enable. Respond only with JSON.'''\n",
    "\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525fe034",
   "metadata": {},
   "source": [
    "## 7. LLM Adjustment Callback\n",
    "Function to call the LLM and parse its JSON response. Falls back to random config if parsing fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefbffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_adjust_callback(data, model_name=\"llama3.2:11b\", custom_prompt=None):\n",
    "    # Use default prompt if none provided\n",
    "    prompt = custom_prompt or prompt_template.format(\n",
    "        blur_reduction=data['metrics']['blur_reduction'],\n",
    "        ocr_gain=data['metrics']['ocr_gain'],\n",
    "        score=data['metrics']['score'],\n",
    "        params_json=json.dumps(data['params'], indent=2)\n",
    "    )\n",
    "    response = chat(model=model_name, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    try:\n",
    "        return json.loads(response['message']['content'])\n",
    "    except Exception:\n",
    "        return generate_random_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2382c3eb",
   "metadata": {},
   "source": [
    "## 8. Image Quality Evaluation\n",
    "Compute blur variance and OCR text length to score improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a90331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_image_quality(orig, proc):\n",
    "    b_orig = cv2.Laplacian(cv2.cvtColor(orig, cv2.COLOR_BGR2GRAY), cv2.CV_64F).var()\n",
    "    b_proc = cv2.Laplacian(cv2.cvtColor(proc, cv2.COLOR_BGR2GRAY), cv2.CV_64F).var()\n",
    "    text_orig = len(pytesseract.image_to_string(orig))\n",
    "    text_proc = len(pytesseract.image_to_string(proc))\n",
    "    return {\n",
    "        'blur_reduction': b_proc - b_orig,\n",
    "        'ocr_gain': text_proc - text_orig,\n",
    "        'score': (b_proc - b_orig) + (text_proc - text_orig)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f09d85",
   "metadata": {},
   "source": [
    "## 9. Pipeline Optimization Loop\n",
    "Iteratively sample configurations and use the LLM feedback to find the best pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af886bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_pipeline(orig_image, param_space, llm_call, max_iter=3):\n",
    "    best = {'score': -float('inf')}\n",
    "    for _ in range(max_iter):\n",
    "        # Randomly sample parameters\n",
    "        current_params = {\n",
    "            k: {sk: random.choice(v) for sk, v in sp.items()}\n",
    "            for k, sp in param_space.items()\n",
    "        }\n",
    "        # Enable all steps initially\n",
    "        config = {\n",
    "            \"use\": dict.fromkeys(current_params.keys(), True),\n",
    "            \"params\": current_params\n",
    "        }\n",
    "        steps = build_pipeline(config)\n",
    "        img, _ = process_image(orig_image, steps)\n",
    "        metrics = evaluate_image_quality(orig_image, img)\n",
    "\n",
    "        # Get LLM-adjusted config\n",
    "        new_config = llm_call({\n",
    "            \"orig\": orig_image,\n",
    "            \"proc\": img,\n",
    "            \"metrics\": metrics,\n",
    "            \"params\": current_params\n",
    "        })\n",
    "        new_steps = build_pipeline(new_config)\n",
    "        new_img, history = process_image(orig_image, new_steps)\n",
    "        new_metrics = evaluate_image_quality(orig_image, new_img)\n",
    "\n",
    "        # Update best\n",
    "        if new_metrics['score'] > best['score']:\n",
    "            best.update({\n",
    "                'image': new_img,\n",
    "                'config': new_config,\n",
    "                'metrics': new_metrics,\n",
    "                'history': history,\n",
    "                'score': new_metrics['score']\n",
    "            })\n",
    "    return best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e9830e",
   "metadata": {},
   "source": [
    "## 10. Running Optimization and Visualizing Results\n",
    "Apply the optimization to an example image and display before/after along with step history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb06a06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load example image\n",
    "orig = cv2.imread('path_to_example_image.jpg')\n",
    "best_result = optimize_pipeline(orig, param_space, llm_adjust_callback)\n",
    "\n",
    "print(\"Best Configuration:\\n\", json.dumps(best_result['config'], indent=2))\n",
    "print(\"Metrics:\\n\", best_result['metrics'])\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(cv2.cvtColor(orig, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Original\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(cv2.cvtColor(best_result['image'], cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Optimized by LLM\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# History of applied steps\n",
    "for entry in best_result['history']:\n",
    "    print(entry)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
